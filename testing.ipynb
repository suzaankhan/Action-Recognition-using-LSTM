{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68f0d021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 2, 2)\n",
      "[[[[0.02152952 0.84722016]\n",
      "   [0.72626037 0.16949886]]\n",
      "\n",
      "  [[0.91723579 0.70457952]\n",
      "   [0.85333435 0.32380562]]\n",
      "\n",
      "  [[0.87306057 0.38414998]\n",
      "   [0.5763713  0.78053359]]]\n",
      "\n",
      "\n",
      " [[[0.33307525 0.99618028]\n",
      "   [0.03616348 0.57886288]]\n",
      "\n",
      "  [[0.28117026 0.4539818 ]\n",
      "   [0.07724965 0.50027364]]\n",
      "\n",
      "  [[0.56320437 0.06115465]\n",
      "   [0.63197558 0.6327468 ]]]]\n",
      "0.8533343533510791\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example dimensions\n",
    "num_videos = 2\n",
    "num_frames_per_video = 3\n",
    "num_body_parts = 2  # Assuming there are 2 body parts\n",
    "num_coordinates = 2  # X and Y coordinates\n",
    "\n",
    "# Initialize an empty 4D array\n",
    "landmarks_array = np.zeros((num_videos, num_frames_per_video, num_body_parts, num_coordinates))\n",
    "\n",
    "# Fill the array with random landmark coordinates\n",
    "for video in range(num_videos):\n",
    "    for frame in range(num_frames_per_video):\n",
    "        for body_part in range(num_body_parts):\n",
    "            # Generate random landmark coordinates for demonstration\n",
    "            landmarks = np.random.rand(num_coordinates)\n",
    "            landmarks_array[video, frame, body_part, :] = landmarks\n",
    "\n",
    "print(landmarks_array.shape)\n",
    "print(landmarks_array)\n",
    "print(landmarks_array[0][1][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ad84a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5, 3, 2, 2)\n",
      "y_train shape: (5,)\n",
      "[[[[0.53045525 0.37590549]\n",
      "   [0.27882144 0.14017687]]\n",
      "\n",
      "  [[0.26054077 0.85413362]\n",
      "   [0.13420446 0.74306258]]\n",
      "\n",
      "  [[0.49307652 0.0420568 ]\n",
      "   [0.72568343 0.58097617]]]\n",
      "\n",
      "\n",
      " [[[0.71730552 0.46572424]\n",
      "   [0.6456634  0.28387456]]\n",
      "\n",
      "  [[0.71395527 0.91484056]\n",
      "   [0.17754225 0.06993347]]\n",
      "\n",
      "  [[0.3466766  0.77398984]\n",
      "   [0.45186156 0.76447896]]]\n",
      "\n",
      "\n",
      " [[[0.07299232 0.26525719]\n",
      "   [0.23608832 0.78667421]]\n",
      "\n",
      "  [[0.78638702 0.458612  ]\n",
      "   [0.57113127 0.36204022]]\n",
      "\n",
      "  [[0.18584336 0.56742608]\n",
      "   [0.31359595 0.94528516]]]\n",
      "\n",
      "\n",
      " [[[0.20448402 0.98674582]\n",
      "   [0.21313213 0.87669688]]\n",
      "\n",
      "  [[0.93359215 0.7495475 ]\n",
      "   [0.88223263 0.2411046 ]]\n",
      "\n",
      "  [[0.5358757  0.99910318]\n",
      "   [0.22220082 0.82799158]]]\n",
      "\n",
      "\n",
      " [[[0.61517742 0.03848384]\n",
      "   [0.64674867 0.95852676]]\n",
      "\n",
      "  [[0.71845278 0.44830303]\n",
      "   [0.99678073 0.95393931]]\n",
      "\n",
      "  [[0.38176184 0.89856114]\n",
      "   [0.20413263 0.12167254]]]]\n",
      "[0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define input dimensions\n",
    "num_videos = 5  # Number of videos for the same action\n",
    "num_frames_per_video = 3  # Number of frames per video\n",
    "num_body_parts = 2  # Assuming there are 5 body parts\n",
    "num_coordinates = 2  # X and Y coordinates\n",
    "num_classes = 2  # Assuming there are 4 action classes\n",
    "\n",
    "# Generate example data (replace with your actual data)\n",
    "X_train = np.random.rand(num_videos, num_frames_per_video, num_body_parts, num_coordinates)\n",
    "y_train = np.random.randint(0, num_classes, size=(num_videos,))\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)  # (num_videos, num_frames_per_video, num_body_parts, num_coordinates)\n",
    "print(\"y_train shape:\", y_train.shape)  # (num_videos,)\n",
    "\n",
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f37b61cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f4d03e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0. 1.]\n",
      "   [0. 1.]]\n",
      "\n",
      "  [[0. 1.]\n",
      "   [0. 1.]]\n",
      "\n",
      "  [[0. 1.]\n",
      "   [0. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 1.]\n",
      "   [0. 1.]]\n",
      "\n",
      "  [[0. 1.]\n",
      "   [0. 1.]]\n",
      "\n",
      "  [[0. 1.]\n",
      "   [0. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 1.]\n",
      "   [0. 1.]]\n",
      "\n",
      "  [[0. 1.]\n",
      "   [0. 1.]]\n",
      "\n",
      "  [[0. 1.]\n",
      "   [0. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 1.]\n",
      "   [0. 1.]]\n",
      "\n",
      "  [[0. 1.]\n",
      "   [0. 1.]]\n",
      "\n",
      "  [[0. 1.]\n",
      "   [0. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 1.]\n",
      "   [0. 1.]]\n",
      "\n",
      "  [[0. 1.]\n",
      "   [0. 1.]]\n",
      "\n",
      "  [[0. 1.]\n",
      "   [0. 1.]]]]\n",
      "[0 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_videos = 5  # Number of videos for the same action\n",
    "num_frames_per_video = 3  # Number of frames per video\n",
    "num_body_parts = 2  # Assuming there are 5 body parts\n",
    "num_coordinates = 2  # X and Y coordinates\n",
    "num_classes = 2  # Assuming there are 4 action classes\n",
    "\n",
    "train_videos = np.zeros((num_videos, num_frames_per_video, num_body_parts, num_coordinates))\n",
    "output = np.zeros(num_videos, dtype=int)\n",
    "\n",
    "for video in range(num_videos):\n",
    "    for frame in range(num_frames_per_video):\n",
    "        for body_part in range(num_body_parts):\n",
    "            for coordinate in range(num_coordinates):\n",
    "                landmark = coordinate\n",
    "                train_videos[video,frame, body_part,coordinate] = landmark\n",
    "\n",
    "for video in range(num_videos):\n",
    "    output[video] = np.random.randint(num_classes)\n",
    "    \n",
    "print(train_videos)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dae7f3b",
   "metadata": {},
   "source": [
    "# Data Collection where each frame is labelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf0e0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "no_of_frames = 10\n",
    "no_of_body_parts = 33\n",
    "no_of_coordinated = 3\n",
    "\n",
    "train_x = np.zeros((no_of_frames, no_of_body_parts, no_of_coordinated))\n",
    "\n",
    "for frame in range(no_of_frames):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bedb7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video ended\n",
      "Total Frames : 47\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "video_path = 'fall.mp4'\n",
    "\n",
    "video = cv2.VideoCapture('fall.mp4')\n",
    "total_frames = 0\n",
    "if not video.isOpened():\n",
    "    print(\"failed to open video\")\n",
    "\n",
    "while True:\n",
    "    success, frame = video.read()\n",
    "    \n",
    "    if not success:\n",
    "        print(\"Video ended\")\n",
    "        break\n",
    "        \n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    total_frames += 1\n",
    "    \n",
    "    if cv2.waitKey(50) == ord('q'):\n",
    "        break\n",
    "        \n",
    "video.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(f'Total Frames : {total_frames}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c19ee2f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:971: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'cv::imshow'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1124\\2334092203.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mvideo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCAP_PROP_POS_FRAMES\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvideo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'frame'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mvideo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:971: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'cv::imshow'\n"
     ]
    }
   ],
   "source": [
    "video.set(cv2.CAP_PROP_POS_FRAMES, 3)\n",
    "ret , frame = video.read()\n",
    "cv2.imshow('frame', frame)\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddeb85cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fd28f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n",
      "(20, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6],\n",
       "       [7, 8, 9],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_with_less_frames = np.array([[1,2,3], [4,5,6], [7,8,9]])\n",
    "desired_lenght = 20\n",
    "print(array_with_less_frames.shape)\n",
    "no_of_padding = desired_lenght - len(array_with_less_frames)\n",
    "\n",
    "array_with_all_frames = np.pad(array_with_less_frames, ((0, no_of_padding), (0,0)), mode='constant', constant_values=0)\n",
    "\n",
    "print(array_with_all_frames.shape)\n",
    "array_with_all_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1b78fc",
   "metadata": {},
   "source": [
    "# To obtain number of frames in a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "388a60c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8eb73760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_path = 'fall.mp4'\n",
    "\n",
    "video = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not video.isOpened():\n",
    "    print(\"error opening video\")\n",
    "\n",
    "no_of_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "video.release()\n",
    "\n",
    "no_of_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5153e278",
   "metadata": {},
   "source": [
    "# To obtain list of 30 equally seperated indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "276986fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  3  4  6  7  9 11 12 14 15 17 19 20 22 23 25 26 28 30 31 33 34 36\n",
      " 38 39 41 42 44 46]\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "needed_frames = 30\n",
    "\n",
    "no_of_frame_indexes = np.linspace(0, no_of_frames - 1, needed_frames, dtype=int)\n",
    "print(no_of_frame_indexes)\n",
    "print(len(no_of_frame_indexes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3b2aaa",
   "metadata": {},
   "source": [
    "# To access frames from indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4649ba6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n",
      "[ 0  1  3  4  6  7  9 11 12 14 15 17 19 20 22 23 25 26 28 30 31 33 34 36\n",
      " 38 39 41 42 44 46]\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "video_path = 'fall.mp4'\n",
    "\n",
    "video = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not video.isOpened():\n",
    "    print(\"error opening video\")\n",
    "\n",
    "needed_frames = 30\n",
    "    \n",
    "no_of_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "no_of_frame_indexes = np.linspace(0, no_of_frames - 1, needed_frames, dtype=int)\n",
    "print(no_of_frames)\n",
    "print(no_of_frame_indexes)\n",
    "print(len(no_of_frame_indexes))\n",
    "\n",
    "\n",
    "for frame_index in no_of_frame_indexes:\n",
    "    video.set(cv2.CAP_PROP_POS_FRAMES, frame_index)\n",
    "    \n",
    "    success, frame = video.read()\n",
    "    \n",
    "    if not success:\n",
    "        break\n",
    "    cv2.imshow(\"frame\", frame)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "    \n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013dfb1d",
   "metadata": {},
   "source": [
    "# Detecting landmarks on a video (Trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03c0e019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280 720\n",
      "x: 0.7408503293991089\n",
      "y: 0.3202008605003357\n",
      "z: -0.48209160566329956\n",
      "visibility: 0.9999697804450989\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "video_file = 'testing_video.mp4'\n",
    "\n",
    "cap = cv2.VideoCapture(video_file)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening the video\")\n",
    "\n",
    "#resizing the frame\n",
    "_, frame = cap.read()\n",
    "h, w = frame.shape[0], frame.shape[1]\n",
    "print(h, w)\n",
    "cv2.namedWindow('Window Name', cv2.WINDOW_NORMAL)  # WINDOW_NORMAL allows resizing\n",
    "# Resize the window\n",
    "cv2.resizeWindow('Window Name', w//2, h//2)  # Set the desired width and height\n",
    "\n",
    "while True:\n",
    "    success, frame = cap.read()\n",
    "    \n",
    "    if not success:\n",
    "        print(\"Video ended\")\n",
    "        break\n",
    "        \n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(frame_rgb)\n",
    "    \n",
    "    mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "    \n",
    "    cv2.imshow(\"Window Name\", frame)\n",
    "#     print(frame.shape)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "if results.pose_landmarks:\n",
    "    print(results.pose_landmarks.landmark[mp_pose.PoseLandmark(0).value])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0424c1ab",
   "metadata": {},
   "source": [
    "## 1. Create a function that iterates through a folder that has all video data\n",
    "## 2. For each video get landmarks for each frame and append it to an array say training_data\n",
    "## 3. pre-process each frame array (padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9f5d392",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling getTrainingData\n",
      "t_1.mp4\n",
      "Failed to open video\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2864\\547685116.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;31m##########################################################################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"calling getTrainingData\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m \u001b[0mgetTrainingData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[0mtraining_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2864\\547685116.py\u001b[0m in \u001b[0;36mgetTrainingData\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;31m#resizing the window\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamedWindow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'frame'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWINDOW_NORMAL\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# WINDOW_NORMAL allows resizing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# creating mediapipe objects\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.4, min_tracking_confidence=0.4, model_complexity=2)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "#defining no of frames we need to depict one action\n",
    "frames_needed = 30\n",
    "directory = 'data/'\n",
    "#define training data array, here we will store all data array for one action\n",
    "training_data = []\n",
    "# training_data = np.array(training_data)\n",
    "counter = 0\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "# function to get each frame\n",
    "def getLandmarksArrayForFrame(results):\n",
    "    landmarks = []\n",
    "    \n",
    "    for i in range(33):\n",
    "        landmarks.append([results.pose_landmarks.landmark[mp_pose.PoseLandmark(i).value].x,\n",
    "                         results.pose_landmarks.landmark[mp_pose.PoseLandmark(i).value].y,\n",
    "                         results.pose_landmarks.landmark[mp_pose.PoseLandmark(i).value].visibility])\n",
    "    \n",
    "    landmarks = np.array(landmarks)\n",
    "    return landmarks\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "def getTrainingData(directory):\n",
    "    # Reading each video and collecting data from it present in data folder\n",
    "    for video in os.listdir(directory):\n",
    "        print(video)\n",
    "        # in this array we will add input of shape (30, 33, 3) of each video and then append it to training_data array\n",
    "        frame_array = []\n",
    "\n",
    "        cap = cv2.VideoCapture(video)\n",
    "        if not cap.isOpened():\n",
    "            print('Failed to open video')\n",
    "\n",
    "        #resizing the window\n",
    "        _, frame = cap.read()\n",
    "        h, w = frame.shape[0], frame.shape[1]\n",
    "        print(h, w)\n",
    "        cv2.namedWindow('frame', cv2.WINDOW_NORMAL)  # WINDOW_NORMAL allows resizing\n",
    "        # Resize the window\n",
    "        cv2.resizeWindow('frame', w//2, h//2)  # Set the desired width and height\n",
    "\n",
    "        # computing required values\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        step = total_frames // frames_needed\n",
    "        frames_indexes = np.linspace(0, total_frames - 1, frames_needed, dtype=int)\n",
    "\n",
    "        #printing the values computed for verification\n",
    "        print(f'Total frames is {total_frames}')\n",
    "        print(f'step is {step}')\n",
    "        print(frames_indexes)\n",
    "        print(f'Lenght of frames index array is {len(frames_indexes)}')\n",
    "        \n",
    "        getAllFrames(frames_indexes, cap, frame_array)\n",
    "        \n",
    "    cap.release()\n",
    "        \n",
    "##############################################################################################################\n",
    "        \n",
    "def getAllFrames(frames_indexes, cap, frame_array):\n",
    "    global counter\n",
    "    for frame_index in frames_indexes:\n",
    "        \n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)\n",
    "        \n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            print('Video ended')\n",
    "        \n",
    "        # converted to rgb because mediapipe reads images in rgb format\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "        #detecting landmarks on each frame\n",
    "        results = pose.process(rgb_frame)\n",
    "        \n",
    "        #drawing landmarks\n",
    "        mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "        \n",
    "        if results.pose_landmarks:\n",
    "            frame_array.append(getLandmarksArrayForFrame(results))\n",
    "        else:\n",
    "            print(f\"landmarks from one frame not detected, frame no is {frame_index}\")\n",
    "            frame_array.append(np.zeros((33,3), dtype=int))\n",
    "            \n",
    "#         cv2.imshow(\"frame\", frame)\n",
    "        counter += 1\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "            \n",
    "    frame_array = np.array(frame_array)\n",
    "    training_data.append(frame_array)\n",
    "    \n",
    "\n",
    "##########################################################################################################################\n",
    "print(\"calling getTrainingData\")\n",
    "getTrainingData(directory)\n",
    "\n",
    "training_data = np.array(training_data)\n",
    "print(training_data.shape)\n",
    "print(training_data)\n",
    "print(f'Readed {counter} frames')\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e766a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed 'video_1.mp4' to 't_1.mp4'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def rename_videos(directory, prefix=\"t_\", start_index=1):\n",
    "    \n",
    "    # Ensure that the directory exists\n",
    "    if not os.path.isdir(directory):\n",
    "        print(f\"Error: Directory '{directory}' not found.\")\n",
    "        return\n",
    "    \n",
    "    for index, filename in enumerate(os.listdir(directory), start=start_index):\n",
    "        # Check if the file is a video (you can modify this condition if needed)\n",
    "        if filename.endswith(\".mp4\"):\n",
    "            # Construct the new filename with the prefix and index\n",
    "            new_filename = f\"{prefix}{index}.mp4\"\n",
    "            # Rename the file\n",
    "            os.rename(os.path.join(directory, filename), os.path.join(directory, new_filename))\n",
    "            print(f\"Renamed '{filename}' to '{new_filename}'\")\n",
    "\n",
    "# Example usage:\n",
    "directory_path = \"data\"\n",
    "rename_videos(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "953ea877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eror\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:971: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'cv::imshow'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_808\\3596081515.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'frame'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mvid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:971: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'cv::imshow'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "vid = cv2.VideoCapture(\"data/video_30.mp4\")\n",
    "\n",
    "if not vid.isOpened():\n",
    "    print(\"eror\")\n",
    "else:\n",
    "    print(\"Video is opened\")\n",
    "\n",
    "_, frame = vid.read()\n",
    "cv2.imshow('frame', frame)\n",
    "cv2.waitKey(5000)\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b50c327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "for file_name in os.listdir('data/'):\n",
    "    print(file_name)\n",
    "    vid = cv2.VideoCapture(file_name)\n",
    "\n",
    "    if not vid.isOpened():\n",
    "        print(\"eror\")\n",
    "    else:\n",
    "        print(\"Video is opened\")\n",
    "    vid.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "295c6b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (1.9.1)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.12.0-cp39-cp39-win_amd64.whl (46.2 MB)\n",
      "     ---------------------------------------- 46.2/46.2 MB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy<1.29.0,>=1.22.4 in c:\\users\\simrah khan\\appdata\\roaming\\python\\python39\\site-packages (from scipy) (1.26.4)\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47fbc025",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1588\\1880158662.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtraining_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'training_data' is not defined"
     ]
    }
   ],
   "source": [
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b128efe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((33,3), dtype=int)\n",
    "# a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9352f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((3), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1eea68cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data[0][29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75594f7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.81765854, 0.27486673, 0.99384487],\n",
       "       [0.81248319, 0.2572858 , 0.99336207],\n",
       "       [0.81056255, 0.25685763, 0.99422508],\n",
       "       [0.80827594, 0.2565842 , 0.99603856],\n",
       "       [0.82536274, 0.25542882, 0.98978627],\n",
       "       [0.83258897, 0.25356373, 0.98984528],\n",
       "       [0.83982241, 0.25155866, 0.99049711],\n",
       "       [0.82265991, 0.25592601, 0.99899238],\n",
       "       [0.86792797, 0.250718  , 0.9889937 ],\n",
       "       [0.82393038, 0.28683591, 0.996512  ],\n",
       "       [0.83471054, 0.28555879, 0.99175006],\n",
       "       [0.81385827, 0.33010596, 0.99995613],\n",
       "       [0.99654287, 0.32080239, 0.9941082 ],\n",
       "       [0.80519223, 0.47398639, 0.98916513],\n",
       "       [0.94861096, 0.45453531, 0.00676032],\n",
       "       [0.73577124, 0.60478884, 0.9554764 ],\n",
       "       [0.94501179, 0.55172557, 0.00643459],\n",
       "       [0.73883855, 0.64674646, 0.92017543],\n",
       "       [0.94112611, 0.56863534, 0.01053997],\n",
       "       [0.73530978, 0.65012616, 0.91404182],\n",
       "       [0.91760892, 0.56929266, 0.01005783],\n",
       "       [0.73743844, 0.63432831, 0.85003984],\n",
       "       [0.91224945, 0.56461793, 0.01081186],\n",
       "       [0.85999966, 0.59464836, 0.99898666],\n",
       "       [0.98629975, 0.58428633, 0.99636382],\n",
       "       [0.83172667, 0.76895589, 0.96902746],\n",
       "       [1.06741357, 0.6976952 , 0.21869397],\n",
       "       [0.88800108, 0.94477922, 0.96910793],\n",
       "       [1.08925605, 0.83781177, 0.29641321],\n",
       "       [0.91107816, 0.97708338, 0.92340553],\n",
       "       [1.09759712, 0.86906642, 0.45776901],\n",
       "       [0.78712499, 0.93408048, 0.84483087],\n",
       "       [1.05499709, 0.84519732, 0.17515871]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc8abc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_1.mp4\n",
      "Failed to open video\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10360\\4261091067.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;31m#resizing the window\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamedWindow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'frame'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWINDOW_NORMAL\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# WINDOW_NORMAL allows resizing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# creating mediapipe objects\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.4, min_tracking_confidence=0.4, model_complexity=2)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "#defining no of frames we need to depict one action\n",
    "frames_needed = 30\n",
    "directory = 'data'\n",
    "#define training data array, here we will store all data array for one action\n",
    "training_data = []\n",
    "# training_data = np.array(training_data)\n",
    "counter = 0\n",
    "\n",
    "\n",
    "# function to get each frame\n",
    "def getLandmarksArrayForFrame(results):\n",
    "    landmarks = []\n",
    "    \n",
    "    for i in range(33):\n",
    "        landmarks.append([results.pose_landmarks.landmark[mp_pose.PoseLandmark(i).value].x,\n",
    "                         results.pose_landmarks.landmark[mp_pose.PoseLandmark(i).value].y,\n",
    "                         results.pose_landmarks.landmark[mp_pose.PoseLandmark(i).value].visibility])\n",
    "    \n",
    "    landmarks = np.array(landmarks)\n",
    "    return landmarks\n",
    "\n",
    "\n",
    "# Reading each video and collecting data from it present in data folder\n",
    "for video in os.listdir(directory):\n",
    "    print(video)\n",
    "    # in this array we will add input of shape (30, 33, 3) of each video and then append it to training_data array\n",
    "    frame_array = []\n",
    "    \n",
    "    cap = cv2.VideoCapture(video)\n",
    "    if not cap.isOpened():\n",
    "        print('Failed to open video')\n",
    "    \n",
    "    #resizing the window\n",
    "    _, frame = cap.read()\n",
    "    h, w = frame.shape[0], frame.shape[1]\n",
    "    print(h, w)\n",
    "    cv2.namedWindow('frame', cv2.WINDOW_NORMAL)  # WINDOW_NORMAL allows resizing\n",
    "    # Resize the window\n",
    "    cv2.resizeWindow('frame', w//2, h//2)  # Set the desired width and height\n",
    "\n",
    "    # computing required values\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    step = total_frames // frames_needed\n",
    "    frames_indexes = np.linspace(0, total_frames - 1, frames_needed, dtype=int)\n",
    "    \n",
    "    #printing the values computed for verification\n",
    "    print(f'Total frames is {total_frames}')\n",
    "    print(f'step is {step}')\n",
    "    print(frames_indexes)\n",
    "    print(f'Lenght of frames index array is {len(frames_indexes)}')\n",
    "    \n",
    "    # this part is responsible for obtaining landmarks from each frame for a particular frame index\n",
    "    for frame_index in frames_indexes:\n",
    "        \n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)\n",
    "        \n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            print('Video ended')\n",
    "        \n",
    "        # converted to rgb because mediapipe reads images in rgb format\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "        #detecting landmarks on each frame\n",
    "        results = pose.process(rgb_frame)\n",
    "        \n",
    "        #drawing landmarks\n",
    "        mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "        \n",
    "        if results.pose_landmarks:\n",
    "            frame_array.append(getLandmarksArrayForFrame(results))\n",
    "        else:\n",
    "            print(f\"landmarks from one frame not detected, frame no is {frame_index}\")\n",
    "            frame_array.append(np.zeros((33,3), dtype=int))\n",
    "            \n",
    "        cv2.imshow(\"frame\", frame)\n",
    "        counter += 1\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "            \n",
    "    frame_array = np.array(frame_array)\n",
    "    training_data.append(frame_array)\n",
    "    \n",
    "training_data = np.array(training_data)\n",
    "print(training_data.shape)\n",
    "print(training_data)\n",
    "print(f'Readed {counter} frames')\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
