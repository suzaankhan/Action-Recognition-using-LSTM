{
 "cells": [
  {
   "cell_type": "raw",
   "id": "32b2761f",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "prefix = 'video_'\n",
    "count = 1\n",
    "\n",
    "for file_name in os.listdir('data'):\n",
    "    if file_name.endswith(\".mp4\"):\n",
    "        new_file_name = f'{prefix}{count}.mp4'\n",
    "        os.rename(os.path.join( 'data',file_name), os.path.join('data', new_file_name))\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53103fb",
   "metadata": {},
   "source": [
    "# Code to collect Training and Testing Data\n",
    "### Run cell 1 before running cell 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f354979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Simrah Khan\\.conda\\envs\\sem6proj\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# creating mediapipe objects\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.4, min_tracking_confidence=0.4, model_complexity=2)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "#defining no of frames we need to depict one action\n",
    "frames_needed = 30\n",
    "# directory = 'data'\n",
    "#define training data array, here we will store all data array for one action\n",
    "training_data = []\n",
    "\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1064e026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get each frame\n",
    "def getLandmarksArrayForFrame(results):\n",
    "    landmarks = []\n",
    "    \n",
    "    for i in range(33):\n",
    "        landmarks.append(results.pose_landmarks.landmark[mp_pose.PoseLandmark(i).value].x)\n",
    "        landmarks.append(results.pose_landmarks.landmark[mp_pose.PoseLandmark(i).value].y)\n",
    "        landmarks.append(results.pose_landmarks.landmark[mp_pose.PoseLandmark(i).value].visibility)\n",
    "#         landmarks.append([results.pose_landmarks.landmark[mp_pose.PoseLandmark(i).value].x,\n",
    "#                          results.pose_landmarks.landmark[mp_pose.PoseLandmark(i).value].y,\n",
    "#                          results.pose_landmarks.landmark[mp_pose.PoseLandmark(i).value].visibility])\n",
    "    \n",
    "    landmarks = np.array(landmarks)\n",
    "    return landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb926834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainingData(directory):\n",
    "    # Reading each video and collecting data from it present in data folder\n",
    "    for video in os.listdir(directory):\n",
    "        print(video)\n",
    "        # in this array we will add input of shape (30, 33, 3) of each video and then append it to training_data array\n",
    "        frame_array = []\n",
    "\n",
    "        cap = cv2.VideoCapture(f'{directory}/{video}')\n",
    "        if not cap.isOpened():\n",
    "            print('Failed to open video')\n",
    "\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            print(\"cap read failed in getTrainingData method\")\n",
    "        #resizing the window\n",
    "        h, w = frame.shape[0], frame.shape[1]\n",
    "        print(h, w)\n",
    "#         --------------------------------------------------------------------------------------------------\n",
    "        cv2.namedWindow('frame', cv2.WINDOW_NORMAL)  # WINDOW_NORMAL allows resizing\n",
    "        # Resize the window\n",
    "        cv2.resizeWindow('frame', w//2, h//2)  # Set the desired width and height\n",
    "#         --------------------------------------------------------------------------------------------------\n",
    "\n",
    "        # computing required values\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        step = total_frames // frames_needed\n",
    "        frames_indexes = np.linspace(0, total_frames - 1, frames_needed, dtype=int)\n",
    "\n",
    "        #printing the values computed for verification\n",
    "        print(f'Total frames is {total_frames}')\n",
    "        print(f'step is {step}')\n",
    "        print(frames_indexes)\n",
    "        print(f'Lenght of frames index array is {len(frames_indexes)}')\n",
    "        \n",
    "        getAllFrames(frames_indexes, cap, frame_array)\n",
    "        \n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7357f048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllFrames(frames_indexes, cap, frame_array):\n",
    "    global counter\n",
    "    frame_counter = 0\n",
    "    for frame_index in frames_indexes:\n",
    "        \n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)\n",
    "        \n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            print('failed to read frame or video ended so skipping the video')\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frames_indexes[-2])\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                print('failed to read frame or video ended so skipping the video')\n",
    "                \n",
    "\n",
    "        frame_counter += 1\n",
    "        # converted to rgb because mediapipe reads images in rgb format\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "        #detecting landmarks on each frame\n",
    "        results = pose.process(rgb_frame)\n",
    "        \n",
    "        #drawing landmarks\n",
    "#             --------------------------------------------------------------------------------------------------        \n",
    "        mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "#             --------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        if results.pose_landmarks:\n",
    "            frame_array.append(getLandmarksArrayForFrame(results))\n",
    "        else:\n",
    "            print(f\"landmarks from one frame not detected, frame no is {frame_index}\")\n",
    "            frame_array.append(np.zeros((99), dtype=int))\n",
    "#             --------------------------------------------------------------------------------------------------\n",
    "        cv2.imshow(\"frame\", frame)\n",
    "#             --------------------------------------------------------------------------------------------------\n",
    "        counter += 1\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "        print(f\"Read frame {frame_index}\")\n",
    "    print(f'Read {frame_counter} frames')\n",
    "    frame_array = np.array(frame_array)\n",
    "    training_data.append(frame_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a385f032",
   "metadata": {},
   "source": [
    "## Run below code to collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64c68ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling getTrainingData\n",
      "falling_video_1.mp4\n",
      "1280 720\n",
      "Total frames is 50\n",
      "step is 1\n",
      "[ 0  1  3  5  6  8 10 11 13 15 16 18 20 21 23 25 27 28 30 32 33 35 37 38\n",
      " 40 42 43 45 47 49]\n",
      "Lenght of frames index array is 30\n",
      "Read frame 0\n",
      "Read frame 1\n",
      "Read frame 3\n",
      "Read frame 5\n",
      "Read frame 6\n",
      "Read frame 8\n",
      "Read frame 10\n",
      "Read frame 11\n",
      "Read frame 13\n",
      "Read frame 15\n",
      "Read frame 16\n",
      "Read frame 18\n",
      "Read frame 20\n",
      "Read frame 21\n",
      "Read frame 23\n",
      "Read frame 25\n",
      "Read frame 27\n",
      "Read frame 28\n",
      "Read frame 30\n",
      "Read frame 32\n",
      "Read frame 33\n",
      "Read frame 35\n",
      "Read frame 37\n",
      "Read frame 38\n",
      "Read frame 40\n",
      "Read frame 42\n",
      "Read frame 43\n",
      "Read frame 45\n",
      "Read frame 47\n",
      "Read frame 49\n",
      "Read 30 frames\n",
      "falling_video_10.mp4\n",
      "1280 720\n",
      "Total frames is 41\n",
      "step is 1\n",
      "[ 0  1  2  4  5  6  8  9 11 12 13 15 16 17 19 20 22 23 24 26 27 28 30 31\n",
      " 33 34 35 37 38 40]\n",
      "Lenght of frames index array is 30\n",
      "landmarks from one frame not detected, frame no is 0\n",
      "Read frame 0\n",
      "Read frame 1\n",
      "Read frame 2\n",
      "Read frame 4\n",
      "Read frame 5\n",
      "Read frame 6\n",
      "Read frame 8\n",
      "Read frame 9\n",
      "Read frame 11\n",
      "Read frame 12\n",
      "Read frame 13\n",
      "Read frame 15\n",
      "Read frame 16\n",
      "Read frame 17\n",
      "Read frame 19\n",
      "Read frame 20\n",
      "Read frame 22\n",
      "Read frame 23\n",
      "Read frame 24\n",
      "Read frame 26\n",
      "Read frame 27\n",
      "Read frame 28\n",
      "Read frame 30\n",
      "Read frame 31\n",
      "Read frame 33\n",
      "Read frame 34\n",
      "Read frame 35\n",
      "Read frame 37\n",
      "Read frame 38\n",
      "Read frame 40\n",
      "Read 30 frames\n",
      "falling_video_11.mp4\n",
      "1280 720\n",
      "Total frames is 32\n",
      "step is 1\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 31]\n",
      "Lenght of frames index array is 30\n",
      "Read frame 0\n",
      "Read frame 1\n",
      "landmarks from one frame not detected, frame no is 2\n",
      "Read frame 2\n",
      "Read frame 3\n",
      "Read frame 4\n",
      "Read frame 5\n",
      "Read frame 6\n",
      "Read frame 7\n",
      "Read frame 8\n",
      "Read frame 9\n",
      "Read frame 10\n",
      "Read frame 11\n",
      "Read frame 12\n",
      "Read frame 13\n",
      "Read frame 14\n",
      "Read frame 16\n",
      "Read frame 17\n",
      "Read frame 18\n",
      "Read frame 19\n",
      "Read frame 20\n",
      "Read frame 21\n",
      "Read frame 22\n",
      "Read frame 23\n",
      "Read frame 24\n",
      "Read frame 25\n",
      "Read frame 26\n",
      "Read frame 27\n",
      "Read frame 28\n",
      "Read frame 29\n",
      "Read frame 31\n",
      "Read 30 frames\n",
      "falling_video_12.mp4\n",
      "1280 720\n",
      "Total frames is 42\n",
      "step is 1\n",
      "[ 0  1  2  4  5  7  8  9 11 12 14 15 16 18 19 21 22 24 25 26 28 29 31 32\n",
      " 33 35 36 38 39 41]\n",
      "Lenght of frames index array is 30\n",
      "Read frame 0\n",
      "landmarks from one frame not detected, frame no is 1\n",
      "Read frame 1\n",
      "Read frame 2\n",
      "Read frame 4\n",
      "Read frame 5\n",
      "Read frame 7\n",
      "Read frame 8\n",
      "Read frame 9\n",
      "Read frame 11\n",
      "Read frame 12\n",
      "Read frame 14\n",
      "Read frame 15\n",
      "Read frame 16\n",
      "Read frame 18\n",
      "Read frame 19\n",
      "Read frame 21\n",
      "Read frame 22\n",
      "Read frame 24\n",
      "Read frame 25\n",
      "Read frame 26\n",
      "Read frame 28\n",
      "Read frame 29\n",
      "Read frame 31\n",
      "Read frame 32\n",
      "Read frame 33\n",
      "Read frame 35\n",
      "Read frame 36\n",
      "Read frame 38\n",
      "Read frame 39\n",
      "Read frame 41\n",
      "Read 30 frames\n",
      "falling_video_13.mp4\n",
      "1280 720\n",
      "Total frames is 44\n",
      "step is 1\n",
      "[ 0  1  2  4  5  7  8 10 11 13 14 16 17 19 20 22 23 25 26 28 29 31 32 34\n",
      " 35 37 38 40 41 43]\n",
      "Lenght of frames index array is 30\n",
      "Read frame 0\n",
      "Read frame 1\n",
      "Read frame 2\n",
      "Read frame 4\n",
      "Read frame 5\n",
      "Read frame 7\n",
      "Read frame 8\n",
      "Read frame 10\n",
      "Read frame 11\n",
      "Read frame 13\n",
      "Read frame 14\n",
      "Read frame 16\n",
      "Read frame 17\n",
      "Read frame 19\n",
      "Read frame 20\n",
      "Read frame 22\n",
      "Read frame 23\n",
      "Read frame 25\n",
      "Read frame 26\n",
      "Read frame 28\n",
      "landmarks from one frame not detected, frame no is 29\n",
      "Read frame 29\n",
      "Read frame 31\n",
      "Read frame 32\n",
      "Read 24 frames\n",
      "falling_video_14.mp4\n",
      "1280 720\n",
      "Total frames is 42\n",
      "step is 1\n",
      "[ 0  1  2  4  5  7  8  9 11 12 14 15 16 18 19 21 22 24 25 26 28 29 31 32\n",
      " 33 35 36 38 39 41]\n",
      "Lenght of frames index array is 30\n",
      "Read frame 0\n",
      "Read frame 1\n",
      "Read frame 2\n",
      "Read frame 4\n",
      "Read frame 5\n",
      "Read frame 7\n",
      "landmarks from one frame not detected, frame no is 8\n",
      "Read frame 8\n",
      "Read frame 9\n",
      "Read frame 11\n",
      "Read 10 frames\n",
      "falling_video_15.mp4\n",
      "1280 720\n",
      "Total frames is 35\n",
      "step is 1\n",
      "[ 0  1  2  3  4  5  7  8  9 10 11 12 14 15 16 17 18 19 21 22 23 24 25 26\n",
      " 28 29 30 31 32 34]\n",
      "Lenght of frames index array is 30\n",
      "Read frame 0\n",
      "Read frame 1\n",
      "Read frame 2\n",
      "Read frame 3\n",
      "Read frame 4\n",
      "Read frame 5\n",
      "Read frame 7\n",
      "Read frame 8\n",
      "Read frame 9\n",
      "Read frame 10\n",
      "Read frame 11\n",
      "Read frame 12\n",
      "Read frame 14\n",
      "Read frame 15\n",
      "Read frame 16\n",
      "Read frame 17\n",
      "Read frame 18\n",
      "Read frame 19\n",
      "Read frame 21\n",
      "Read frame 22\n",
      "Read 21 frames\n",
      "falling_video_16.mp4\n",
      "1280 720\n",
      "Total frames is 45\n",
      "step is 1\n",
      "[ 0  1  3  4  6  7  9 10 12 13 15 16 18 19 21 22 24 25 27 28 30 31 33 34\n",
      " 36 37 39 40 42 44]\n",
      "Lenght of frames index array is 30\n",
      "landmarks from one frame not detected, frame no is 0\n",
      "Read frame 0\n",
      "Read frame 1\n",
      "Read frame 3\n",
      "Read frame 4\n",
      "Read frame 6\n",
      "Read frame 7\n",
      "Read frame 9\n",
      "Read frame 10\n",
      "Read frame 12\n",
      "Read frame 13\n",
      "Read frame 15\n",
      "Read frame 16\n",
      "Read frame 18\n",
      "Read frame 19\n",
      "Read 15 frames\n",
      "falling_video_17.mp4\n",
      "1280 720\n",
      "Total frames is 43\n",
      "step is 1\n",
      "[ 0  1  2  4  5  7  8 10 11 13 14 15 17 18 20 21 23 24 26 27 28 30 31 33\n",
      " 34 36 37 39 40 42]\n",
      "Lenght of frames index array is 30\n",
      "Read frame 0\n",
      "Read frame 1\n",
      "Read frame 2\n",
      "Read frame 4\n",
      "Read frame 5\n",
      "Read frame 7\n",
      "Read frame 8\n",
      "Read frame 10\n",
      "Read frame 11\n",
      "Read frame 13\n",
      "Read frame 14\n",
      "Read frame 15\n",
      "Read frame 17\n",
      "Read frame 18\n",
      "Read frame 20\n",
      "Read frame 21\n",
      "Read frame 23\n",
      "Read frame 24\n",
      "Read frame 26\n",
      "Read frame 27\n",
      "Read frame 28\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalling getTrainingData\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mgetTrainingData\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata_falling\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# training_data = np.array(training_data)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# print(training_data.shape)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# print(training_data)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(training_data)\n",
      "Cell \u001b[1;32mIn[3], line 35\u001b[0m, in \u001b[0;36mgetTrainingData\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(frames_indexes)\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLenght of frames index array is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(frames_indexes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 35\u001b[0m     \u001b[43mgetAllFrames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes_indexes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_array\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m cap\u001b[38;5;241m.\u001b[39mrelease()\n",
      "Cell \u001b[1;32mIn[4], line 22\u001b[0m, in \u001b[0;36mgetAllFrames\u001b[1;34m(frames_indexes, cap, frame_array)\u001b[0m\n\u001b[0;32m     19\u001b[0m         rgb_frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;66;03m#detecting landmarks on each frame\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[43mpose\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;66;03m#drawing landmarks\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#             --------------------------------------------------------------------------------------------------        \u001b[39;00m\n\u001b[0;32m     26\u001b[0m         mp_drawing\u001b[38;5;241m.\u001b[39mdraw_landmarks(frame, results\u001b[38;5;241m.\u001b[39mpose_landmarks, mp_pose\u001b[38;5;241m.\u001b[39mPOSE_CONNECTIONS)\n",
      "File \u001b[1;32m~\\.conda\\envs\\sem6proj\\lib\\site-packages\\mediapipe\\python\\solutions\\pose.py:185\u001b[0m, in \u001b[0;36mPose.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    165\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\sem6proj\\lib\\site-packages\\mediapipe\\python\\solution_base.py:335\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    329\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    331\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    332\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    333\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 335\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[0;32m    338\u001b[0m solution_outputs \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mnamedtuple(\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSolutionOutputs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"calling getTrainingData\")\n",
    "getTrainingData('data_falling')\n",
    "\n",
    "# training_data = np.array(training_data)\n",
    "# print(training_data.shape)\n",
    "# print(training_data)\n",
    "print(training_data)\n",
    "print(f'Readed {counter} frames')\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a94e491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)\n",
    "# np_training_data = np.array(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f366d486",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_training_data = np.array(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "738683ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 30, 99)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b00da82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36385c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"walking_landmark_data.npy\", np_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8a3af165",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtraining_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c823af01",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_training_data = np.load('landmark_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1f3e5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42, 30, 33, 3)\n"
     ]
    }
   ],
   "source": [
    "print(load_training_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8a5042",
   "metadata": {},
   "source": [
    "# Function to obtain data from a single video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1dd292d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9bdf04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8de8a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3d71d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
